from game.core import SurvivalGame, GameConfig
from game.agents import EnhancedNeuralAgent
import numpy as np
from gwo import GreyWolfOptimizer
import time
import matplotlib.pyplot as plt
import sys
from tqdm import trange

def fitness_function(weights):
    config = GameConfig()
    
    agent = EnhancedNeuralAgent(config, weights)
    scores = []

    for _ in range(3):
        game = SurvivalGame(config, render=False)
        step_count = 0
        max_steps = 500000  
        
        while not game.all_players_dead() and step_count < max_steps:
            state = game.get_state(0, include_internals=True)
            action = agent.predict(state)
            game.update([action])
            step_count += 1
            
        scores.append(game.players[0].score)

    return np.mean(scores)

def test_agent(weights, num_tests=10, render=False):
    config = GameConfig()
    agent = EnhancedNeuralAgent(config, weights)
    scores = []
    wall_deaths = 0
    obstacle_deaths = 0
    
    print(f"\n--- Testando Agente Neural Melhorado ({num_tests} jogos) ---")
    
    for _ in trange(num_tests, desc="Testando agente"):
        game = SurvivalGame(config, render=render)
        
        step_count = 0
        max_steps = 100000000
        
        while not game.all_players_dead() and step_count < max_steps:
            state = game.get_state(0, include_internals=True)
            action = agent.predict(state)
            
            player = game.players[0]
            if player.alive:
                old_y = player.y
            
            game.update([action])
            step_count += 1
            
            if render and game.players[0].alive:
                game.render_frame()
                
        scores.append(game.players[0].score)
       
    bonela_rule_based_genetic_result = [12.69, 16.65, 6.97, 2.79, 15.94, 10.22, 21.90, 4.35, 6.22, 9.95, 19.94, 20.56, 15.74, 17.68, 7.16, 15.68, 2.37, 15.43, 15.13, 22.50, 25.82, 15.85, 17.02, 16.74, 14.69, 11.73, 13.80, 15.13, 12.35, 16.19]
    bonela_neural_agent_genetic_result = [38.32, 54.53, 61.16, 27.55, 16.08, 26.00, 25.33, 18.30, 39.76, 48.17, 44.77, 47.54, 75.43, 23.68, 16.83, 15.81, 67.17, 53.54, 33.59, 49.24, 52.65, 16.35, 44.05, 56.59, 63.23, 43.96, 43.82, 19.19, 28.36, 18.65]
    bonela_human_result = [27.34, 17.63, 39.33, 17.44, 1.16, 24.04, 29.21, 18.92, 25.71, 20.05, 31.88, 15.39, 22.50, 19.27, 26.33, 23.67, 16.82, 28.45, 12.59, 33.01, 21.74, 14.23, 27.90, 24.80, 11.35, 30.12, 17.08, 22.96, 9.41, 35.22]
    
    # Calcular estat√≠sticas dos resultados de refer√™ncia
    bonela_rule_mean = np.mean(bonela_rule_based_genetic_result)
    bonela_neural_mean = np.mean(bonela_neural_agent_genetic_result)
    bonela_human_mean = np.mean(bonela_human_result)
    
    print(f"\nResultados:")
    print(f"Score M√©dio: {np.mean(scores):.2f}")
    print(f"Score M√°ximo: {np.max(scores):.2f}")
    print(f"Score M√≠nimo: {np.min(scores):.2f}")
    print(f"Desvio Padr√£o: {np.std(scores):.2f}")
    
    print(f"\n--- Compara√ß√£o com Benchmarks ---")
    
    # Compara√ß√£o com agente neural do Bonela
    if np.mean(scores) > bonela_neural_mean:
        print(f"üü¢ Agente Neural GWO foi {((np.mean(scores) - bonela_neural_mean) / bonela_neural_mean) * 100:.2f}% melhor que o Agente Neural Gen√©tico do Bonela (m√©dia: {bonela_neural_mean:.2f})")
    else:
        print(f"üî¥ Agente Neural GWO foi {((bonela_neural_mean - np.mean(scores)) / bonela_neural_mean) * 100:.2f}% pior que o Agente Neural Gen√©tico do Bonela (m√©dia: {bonela_neural_mean:.2f})")
    
    # Compara√ß√£o com agente baseado em regras do Bonela
    if np.mean(scores) > bonela_rule_mean:
        print(f"üü¢ Agente Neural GWO foi {((np.mean(scores) - bonela_rule_mean) / bonela_rule_mean) * 100:.2f}% melhor que o Agente Gen√©tico Baseado em Regras do Bonela (m√©dia: {bonela_rule_mean:.2f})")
    else:
        print(f"üî¥ Agente Neural GWO foi {((bonela_rule_mean - np.mean(scores)) / bonela_rule_mean) * 100:.2f}% pior que o Agente Gen√©tico Baseado em Regras do Bonela (m√©dia: {bonela_rule_mean:.2f})")
    
    # Compara√ß√£o com humano do Bonela
    if np.mean(scores) > bonela_human_mean:
        print(f"üü¢ Agente Neural GWO foi {((np.mean(scores) - bonela_human_mean) / bonela_human_mean) * 100:.2f}% melhor que o Humano do Bonela (m√©dia: {bonela_human_mean:.2f})")
    else:
        print(f"üî¥ Agente Neural GWO foi {((bonela_human_mean - np.mean(scores)) / bonela_human_mean) * 100:.2f}% pior que o Humano do Bonela (m√©dia: {bonela_human_mean:.2f})")
    
    # Ranking
    results_dict = {
        "Agente Neural GWO": np.mean(scores),
        "Agente Neural Gen√©tico (Bonela)": bonela_neural_mean,
        "Humano (Bonela)": bonela_human_mean,
        "Agente Gen√©tico Regras (Bonela)": bonela_rule_mean
    }
    
    sorted_results = sorted(results_dict.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n--- Ranking de Desempenho ---")
    for i, (name, score) in enumerate(sorted_results, 1):
        medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
        print(f"{medal} {i}¬∫ lugar: {name} - {score:.2f}")
    
    gwo_position = next(i for i, (name, _) in enumerate(sorted_results, 1) if name == "Agente Neural GWO")
    print(f"\nO Agente Neural GWO ficou em {gwo_position}¬∫ lugar no ranking!")
    
    if wall_deaths + obstacle_deaths > 0:
        print(f"\nCausas de Morte (primeiros 5 testes):")
        print(f"  Mortes por parede: {wall_deaths}")
        print(f"  Mortes por obst√°culo: {obstacle_deaths}")
    
    return scores

def train_gwo():
    """Treina o agente neural melhorado usando GWO"""
    print("\n--- Iniciando Treinamento Neural com GWO ---")
    
    config = GameConfig()
    temp_agent = EnhancedNeuralAgent(config)
    dim = temp_agent.get_weights_count()
    
    print(f"Dimens√µes da rede neural melhorada: {dim} pesos")
    print(f"Arquitetura: {temp_agent.input_size} -> {temp_agent.hidden1_size} -> {temp_agent.hidden2_size} -> {temp_agent.output_size}")
    print(f"Features adicionais: 4 (dist√¢ncia/perigo das paredes)")
    
    gwo = GreyWolfOptimizer(
        fitness_function=fitness_function,
        dim=dim,
        n_wolves=100,  
        max_iter=1000,   
        bounds=(-2.5, 2.5)  
    )
    
    start_time = time.time()
    best_weights, best_score = gwo.optimize()
    end_time = time.time()
    
    print(f"\n--- Treinamento Conclu√≠do ---")
    print(f"Tempo total: {end_time - start_time:.2f} segundos")
    print(f"Melhor Score: {best_score:.2f}")
    
    np.save("best_weights.npy", best_weights)
    print("Melhores pesos salvos em 'best_weights.npy'")
    
    return best_weights, best_score


def demo_agent():
    """Demonstra o agente melhorado treinado"""
    try:
        best_weights = np.load("best_weights.npy")
        print("Carregando pesos do agente melhorado...")
        
        print("\n--- Demonstra√ß√£o Visual do Agente Melhorado ---")
        test_agent(best_weights, num_tests=1, render=True)
        
    except FileNotFoundError:
        print("Arquivo de pesos do agente melhorado n√£o encontrado.")
        print("Execute o treinamento primeiro com: python gwo_trainer.py")

def compare_bonela(weights, num_hyper_tests=10):
    config = GameConfig()
    bonela_rule_based_genetic_result = [12.69, 16.65, 6.97, 2.79, 15.94, 10.22, 21.90, 4.35, 6.22, 9.95, 19.94, 20.56, 15.74, 17.68, 7.16, 15.68, 2.37, 15.43, 15.13, 22.50, 25.82, 15.85, 17.02, 16.74, 14.69, 11.73, 13.80, 15.13, 12.35, 16.19]
    bonela_neural_agent_genetic_result = [38.32, 54.53, 61.16, 27.55, 16.08, 26.00, 25.33, 18.30, 39.76, 48.17, 44.77, 47.54, 75.43, 23.68, 16.83, 15.81, 67.17, 53.54, 33.59, 49.24, 52.65, 16.35, 44.05, 56.59, 63.23, 43.96, 43.82, 19.19, 28.36, 18.65]
    bonela_human_result = [27.34, 17.63, 39.33, 17.44, 1.16, 24.04, 29.21, 18.92, 25.71, 20.05, 31.88, 15.39, 22.50, 19.27, 26.33, 23.67, 16.82, 28.45, 12.59, 33.01, 21.74, 14.23, 27.90, 24.80, 11.35, 30.12, 17.08, 22.96, 9.41, 35.22]
    quantas_vezes_superou_bonela_neural = 0
    quantas_vezes_superou_bonela_rule_based = 0
    quantas_vezes_superou_bonela_human = 0
    for _ in trange(num_hyper_tests, desc="Comparando com o do Bonela (m√©dia de 30 jogos, 10 vezes)"):
        agent = EnhancedNeuralAgent(config, weights)
        scores = []
        for _ in range(30):
            game = SurvivalGame(config, render=False)
            step_count = 0
            max_steps = 100000000
            while not game.all_players_dead() and step_count < max_steps:
                state = game.get_state(0, include_internals=True)
                action = agent.predict(state)
                game.update([action])
                step_count += 1
            scores.append(game.players[0].score)
        if np.mean(scores) > np.mean(bonela_neural_agent_genetic_result):
            quantas_vezes_superou_bonela_neural += 1
        if np.mean(scores) > np.mean(bonela_rule_based_genetic_result):
            quantas_vezes_superou_bonela_rule_based += 1
        if np.mean(scores) > np.mean(bonela_human_result):
            quantas_vezes_superou_bonela_human += 1
    print(f"\nO agente neural melhorado superou o agente do Bonela (Rule Based) em {quantas_vezes_superou_bonela_rule_based / num_hyper_tests * 100:.2f}% das vezes.")
    print(f"O agente neural melhorado superou o agente do Bonela (Neural Agent) em {quantas_vezes_superou_bonela_neural / num_hyper_tests * 100:.2f}% das vezes.")
    print(f"O agente neural melhorado superou o agente do Bonela (Human) em {quantas_vezes_superou_bonela_human / num_hyper_tests * 100:.2f}% das vezes.")


def graph_agent_performance(weights, num_games=30, compare_benchmarks=False):
    """Gera gr√°fico com o desempenho do agente em m√∫ltiplos jogos"""
    config = GameConfig()
    agent = EnhancedNeuralAgent(config, weights)
    scores = []
    
    print(f"\n--- Gerando Gr√°fico de Desempenho ({num_games} jogos) ---")
    
    for game_num in trange(num_games, desc="Executando jogos"):
        game = SurvivalGame(config, render=False)
        step_count = 0
        max_steps = 100000000
        
        while not game.all_players_dead() and step_count < max_steps:
            state = game.get_state(0, include_internals=True)
            action = agent.predict(state)
            game.update([action])
            step_count += 1
            
        scores.append(game.players[0].score)
    
    mean_score = np.mean(scores)
    max_score = np.max(scores)
    min_score = np.min(scores)
    std_score = np.std(scores)
    
    print(f"\n--- Estat√≠sticas de Desempenho ---")
    print(f"M√©dia: {mean_score:.2f}")
    print(f"M√°ximo: {max_score:.2f}")
    print(f"M√≠nimo: {min_score:.2f}")
    print(f"Desvio Padr√£o: {std_score:.2f}")
    
    if compare_benchmarks:
        # Dados dos benchmarks (usando os primeiros 'num_games' resultados se necess√°rio)
        bonela_rule_based_genetic_result = [12.69, 16.65, 6.97, 2.79, 15.94, 10.22, 21.90, 4.35, 6.22, 9.95, 19.94, 20.56, 15.74, 17.68, 7.16, 15.68, 2.37, 15.43, 15.13, 22.50, 25.82, 15.85, 17.02, 16.74, 14.69, 11.73, 13.80, 15.13, 12.35, 16.19]
        bonela_neural_agent_genetic_result = [38.32, 54.53, 61.16, 27.55, 16.08, 26.00, 25.33, 18.30, 39.76, 48.17, 44.77, 47.54, 75.43, 23.68, 16.83, 15.81, 67.17, 53.54, 33.59, 49.24, 52.65, 16.35, 44.05, 56.59, 63.23, 43.96, 43.82, 19.19, 28.36, 18.65]
        bonela_human_result = [27.34, 17.63, 39.33, 17.44, 1.16, 24.04, 29.21, 18.92, 25.71, 20.05, 31.88, 15.39, 22.50, 19.27, 26.33, 23.67, 16.82, 28.45, 12.59, 33.01, 21.74, 14.23, 27.90, 24.80, 11.35, 30.12, 17.08, 22.96, 9.41, 35.22]
        
        # Ajustar os dados dos benchmarks para o n√∫mero de jogos solicitado
        bonela_rule = bonela_rule_based_genetic_result[:num_games] if num_games <= 30 else bonela_rule_based_genetic_result
        bonela_neural = bonela_neural_agent_genetic_result[:num_games] if num_games <= 30 else bonela_neural_agent_genetic_result
        bonela_human = bonela_human_result[:num_games] if num_games <= 30 else bonela_human_result
        
        # Criar figura comparativa mais detalhada
        plt.figure(figsize=(15, 12))
        
        # Gr√°fico 1: Compara√ß√£o de desempenho linha por linha
        plt.subplot(3, 2, 1)
        games_range = range(1, len(scores) + 1)
        plt.plot(games_range, scores, 'b-', marker='o', markersize=5, linewidth=2, label='Agente Neural GWO', alpha=0.8)
        
        if len(bonela_neural) >= len(scores):
            plt.plot(games_range, bonela_neural[:len(scores)], 'r-', marker='s', markersize=4, linewidth=2, label='Neural Gen√©tico (Bonela)', alpha=0.7)
        if len(bonela_human) >= len(scores):
            plt.plot(games_range, bonela_human[:len(scores)], 'g-', marker='^', markersize=4, linewidth=2, label='Humano (Bonela)', alpha=0.7)
        if len(bonela_rule) >= len(scores):
            plt.plot(games_range, bonela_rule[:len(scores)], 'orange', marker='d', markersize=4, linewidth=2, label='Regras Gen√©tico (Bonela)', alpha=0.7)
        
        plt.title('Compara√ß√£o de Desempenho por Jogo', fontsize=12, fontweight='bold')
        plt.xlabel('N√∫mero do Jogo')
        plt.ylabel('Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Gr√°fico 2: Box Plot comparativo
        plt.subplot(3, 2, 2)
        data_to_plot = [scores]
        labels = ['GWO Neural']
        
        if len(bonela_neural) >= len(scores):
            data_to_plot.append(bonela_neural[:len(scores)])
            labels.append('Neural Gen.')
        if len(bonela_human) >= len(scores):
            data_to_plot.append(bonela_human[:len(scores)])
            labels.append('Humano')
        if len(bonela_rule) >= len(scores):
            data_to_plot.append(bonela_rule[:len(scores)])
            labels.append('Regras Gen.')
        
        box_plot = plt.boxplot(data_to_plot, labels=labels, patch_artist=True)
        colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightsalmon']
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
        
        plt.title('Distribui√ß√£o dos Scores', fontsize=12, fontweight='bold')
        plt.ylabel('Score')
        plt.grid(True, alpha=0.3)
        
        # Gr√°fico 3: Histograma comparativo
        plt.subplot(3, 2, 3)
        plt.hist(scores, bins=15, alpha=0.7, color='blue', label='GWO Neural', edgecolor='black')
        if len(bonela_neural) >= len(scores):
            plt.hist(bonela_neural[:len(scores)], bins=15, alpha=0.5, color='red', label='Neural Gen.', edgecolor='black')
        plt.axvline(x=mean_score, color='blue', linestyle='--', alpha=0.8, label=f'M√©dia GWO: {mean_score:.2f}')
        plt.title('Distribui√ß√£o - GWO vs Neural Gen√©tico', fontsize=12)
        plt.xlabel('Score')
        plt.ylabel('Frequ√™ncia')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Gr√°fico 4: Barras com m√©dias
        plt.subplot(3, 2, 4)
        methods = ['GWO Neural']
        means = [mean_score]
        stds = [std_score]
        colors_bar = ['blue']
        
        if len(bonela_neural) >= len(scores):
            methods.append('Neural Gen.')
            means.append(np.mean(bonela_neural[:len(scores)]))
            stds.append(np.std(bonela_neural[:len(scores)]))
            colors_bar.append('red')
        if len(bonela_human) >= len(scores):
            methods.append('Humano')
            means.append(np.mean(bonela_human[:len(scores)]))
            stds.append(np.std(bonela_human[:len(scores)]))
            colors_bar.append('green')
        if len(bonela_rule) >= len(scores):
            methods.append('Regras Gen.')
            means.append(np.mean(bonela_rule[:len(scores)]))
            stds.append(np.std(bonela_rule[:len(scores)]))
            colors_bar.append('orange')
        
        bars = plt.bar(methods, means, yerr=stds, capsize=5, color=colors_bar, alpha=0.7, edgecolor='black')
        plt.title('Compara√ß√£o de M√©dias', fontsize=12, fontweight='bold')
        plt.ylabel('Score M√©dio')
        plt.xticks(rotation=45)
        
        # Adicionar valores nas barras
        for bar, mean_val in zip(bars, means):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{mean_val:.1f}', 
                    ha='center', va='bottom', fontweight='bold')
        
        plt.grid(True, alpha=0.3, axis='y')
        
        # Gr√°fico 5: M√©dia m√≥vel
        plt.subplot(3, 2, 5)
        window_size = min(5, len(scores)//2) if len(scores) > 5 else 2
        
        # Calcular m√©dias m√≥veis
        gwo_moving_avg = [np.mean(scores[max(0, i-window_size+1):i+1]) for i in range(len(scores))]
        plt.plot(games_range, gwo_moving_avg, 'b-', linewidth=3, label=f'GWO (m√©dia m√≥vel {window_size})')
        
        if len(bonela_neural) >= len(scores):
            neural_moving_avg = [np.mean(bonela_neural[max(0, i-window_size+1):i+1]) for i in range(len(scores))]
            plt.plot(games_range, neural_moving_avg, 'r-', linewidth=2, label=f'Neural Gen. (m√©dia m√≥vel {window_size})')
        
        plt.title('Tend√™ncia de Desempenho (M√©dia M√≥vel)', fontsize=12, fontweight='bold')
        plt.xlabel('N√∫mero do Jogo')
        plt.ylabel('Score (M√©dia M√≥vel)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Gr√°fico 6: Estat√≠sticas resumo
        plt.subplot(3, 2, 6)
        plt.axis('off')
        
        # Criar tabela de estat√≠sticas
        stats_text = f"""
ESTAT√çSTICAS COMPARATIVAS

Agente Neural GWO:
‚Ä¢ M√©dia: {np.mean(scores):.2f}
‚Ä¢ Mediana: {np.median(scores):.2f}
‚Ä¢ Desvio Padr√£o: {np.std(scores):.2f}
‚Ä¢ M√≠n/M√°x: {np.min(scores):.1f}/{np.max(scores):.1f}

Neural Gen√©tico (Bonela):
‚Ä¢ M√©dia: {np.mean(bonela_neural[:len(scores)]):.2f}
‚Ä¢ Mediana: {np.median(bonela_neural[:len(scores)]):.2f}
‚Ä¢ Desvio Padr√£o: {np.std(bonela_neural[:len(scores)]):.2f}
‚Ä¢ M√≠n/M√°x: {np.min(bonela_neural[:len(scores)]):.1f}/{np.max(bonela_neural[:len(scores)]):.1f}

Humano (Bonela):
‚Ä¢ M√©dia: {np.mean(bonela_human[:len(scores)]):.2f}
‚Ä¢ Mediana: {np.median(bonela_human[:len(scores)]):.2f}
‚Ä¢ Desvio Padr√£o: {np.std(bonela_human[:len(scores)]):.2f}
‚Ä¢ M√≠n/M√°x: {np.min(bonela_human[:len(scores)]):.1f}/{np.max(bonela_human[:len(scores)]):.1f}

Regras Gen√©tico (Bonela):
‚Ä¢ M√©dia: {np.mean(bonela_rule[:len(scores)]):.2f}
‚Ä¢ Mediana: {np.median(bonela_rule[:len(scores)]):.2f}
‚Ä¢ Desvio Padr√£o: {np.std(bonela_rule[:len(scores)]):.2f}
‚Ä¢ M√≠n/M√°x: {np.min(bonela_rule[:len(scores)]):.1f}/{np.max(bonela_rule[:len(scores)]):.1f}
        """
        
        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
        
        plt.tight_layout()
        filename = f'agent_comparison_{num_games}_games.png'
        
    else:
        # Gr√°fico simples original (sem compara√ß√£o)
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 1, 1)
        plt.plot(range(1, num_games + 1), scores, 'b-', marker='o', markersize=4, linewidth=1.5)
        plt.axhline(y=mean_score, color='r', linestyle='--', alpha=0.7, label=f'M√©dia: {mean_score:.2f}')
        plt.axhline(y=max_score, color='g', linestyle='--', alpha=0.7, label=f'M√°ximo: {max_score:.2f}')
        plt.axhline(y=min_score, color='orange', linestyle='--', alpha=0.7, label=f'M√≠nimo: {min_score:.2f}')
        
        plt.title(f'Desempenho do Agente Neural - {num_games} Jogos', fontsize=14, fontweight='bold')
        plt.xlabel('N√∫mero do Jogo')
        plt.ylabel('Score')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.subplot(2, 1, 2)
        plt.hist(scores, bins=min(15, num_games//2), alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(x=mean_score, color='r', linestyle='--', alpha=0.7, label=f'M√©dia: {mean_score:.2f}')
        plt.title('Distribui√ß√£o dos Scores', fontsize=12)
        plt.xlabel('Score')
        plt.ylabel('Frequ√™ncia')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        filename = f'agent_performance_{num_games}_games.png'
    
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nGr√°fico salvo como: {filename}")
    
    plt.show()
    
    return scores


if __name__ == "__main__":
    
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "demo":
            demo_agent()
        elif sys.argv[1] == "test":
            if len(sys.argv) > 2:
                num_tests = int(sys.argv[2])
            else:
                num_tests = 30
            try:
                weights = np.load("best_weights.npy")
                test_agent(weights, num_tests=num_tests, render=False)
            except FileNotFoundError:
                print("Arquivo de pesos n√£o encontrado.")
        elif sys.argv[1] == "compare":
            weights = np.load("best_weights.npy")
            compare_bonela(weights)
        elif sys.argv[1] == "graph":
            compare_mode = False
            num_games = 30
            
            # Verificar argumentos adicionais
            if len(sys.argv) > 2:
                if sys.argv[2] == "compare":
                    compare_mode = True
                    num_games = int(sys.argv[3]) if len(sys.argv) > 3 else 30
                else:
                    try:
                        num_games = int(sys.argv[2])
                        compare_mode = sys.argv[3] == "compare" if len(sys.argv) > 3 else False
                    except ValueError:
                        if sys.argv[2] == "compare":
                            compare_mode = True
                        else:
                            print("Argumento inv√°lido. Use: python gwo_trainer.py graph [num_jogos] [compare] ou python gwo_trainer.py graph compare [num_jogos]")
                            exit(1)
            
            try:
                weights = np.load("best_weights.npy")
                if compare_mode:
                    print("Modo de compara√ß√£o ativado - gerando gr√°ficos comparativos detalhados")
                graph_agent_performance(weights, num_games=num_games, compare_benchmarks=compare_mode)
            except FileNotFoundError:
                print("Arquivo de pesos n√£o encontrado. Execute o treinamento primeiro.")
            except ImportError:
                print("matplotlib n√£o encontrado. Instale com: pip install matplotlib")
        else:
            print("Uso: python gwo_trainer.py [demo|test|compare|graph] [op√ß√µes]")
            print("  demo: Demonstra√ß√£o visual do agente")
            print("  test [n]: Testa o agente n vezes (padr√£o: 30)")
            print("  compare: Compara com resultados do Bonela")
            print("  graph [n]: Gera gr√°fico simples com n jogos (padr√£o: 30)")
            print("  graph compare [n]: Gera gr√°ficos comparativos com benchmarks (padr√£o: 30)")
            print("  graph [n] compare: Alternativa para gr√°ficos comparativos")
    else:
        train_gwo()
